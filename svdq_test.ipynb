{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f88ea33-4483-40a5-af5f-fb8f06710f21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e3bc7e9-cf93-4217-8e7e-a432288b94f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "os.environ['HF_HOME'] = '/root/autodl-tmp/huggingface'\n",
    "os.environ['MODELSCOPE_CACHE'] = '/root/autodl-tmp/modelscope/hub'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d757cc4-093c-4e79-ba82-1607866a057b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ba77588-336b-42da-bdab-cd2bfa9f527e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from diffusers import FluxTransformer2DModel\n",
    "import torch\n",
    "\n",
    "\n",
    "def load_flux_model(\n",
    "    model_path: str,\n",
    "    load_from_file: bool = True,\n",
    "    dtype: torch.dtype = torch.bfloat16,\n",
    ") -> FluxTransformer2DModel:\n",
    "    \"\"\"\n",
    "    加载FLUX模型，支持从单文件或预训练目录加载\n",
    "\n",
    "    参数:\n",
    "        model_path: 模型路径，可以是safetensors文件路径或预训练模型目录\n",
    "        load_from_file: 是否从单个文件加载\n",
    "        dtype: 模型计算精度\n",
    "    \"\"\"\n",
    "    quantization_config = None\n",
    "\n",
    "    if load_from_file:\n",
    "        model = FluxTransformer2DModel.from_single_file(\n",
    "            model_path, quantization_config=quantization_config, torch_dtype=dtype\n",
    "        )\n",
    "    else:\n",
    "        model = FluxTransformer2DModel.from_pretrained(\n",
    "            model_path, quantization_config=quantization_config, torch_dtype=dtype\n",
    "        )\n",
    "\n",
    "    return model\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "ckpt_repo = \"Kijai/flux-fp8\"\n",
    "ckpt_filename = \"flux1-dev-fp8-e4m3fn.safetensors\"\n",
    "\n",
    "ckpt_path = hf_hub_download(ckpt_repo, filename=ckpt_filename)\n",
    "\n",
    "model = load_flux_model(ckpt_path, )\n",
    "\n",
    "from nunchaku.models.flux  import  load_quantized_model\n",
    "\n",
    "qmodel_path = \"mit-han-lab/svdquant-models/svdq-int4-flux.1-dev.safetensors\"\n",
    "\n",
    "if not os.path.exists(qmodel_path):\n",
    "    hf_repo_id = os.path.dirname(qmodel_path)\n",
    "    filename = os.path.basename(qmodel_path)\n",
    "    qmodel_path = hf_hub_download(repo_id=hf_repo_id, filename=filename)\n",
    "\n",
    "\n",
    "m = load_quantized_model(\n",
    "        qmodel_path, \"cuda\"\n",
    "    )\n",
    "\n",
    "from nunchaku.models.flux  import  NunchakuFluxModel, EmbedND, QuantizedFluxModel, SVD_RANK\n",
    "import types\n",
    "\n",
    "def inject_transformer(\n",
    "    transformer_model: FluxTransformer2DModel, m: QuantizedFluxModel\n",
    ") -> None:\n",
    "    \"\"\"注入自定义transformer模型\n",
    "\n",
    "    Args:\n",
    "        transformer_model: 原始transformer模型\n",
    "        custom_model: 要注入的自定义模型\n",
    "    \"\"\"\n",
    "    # 注入位置编码\n",
    "    transformer_model.pos_embed = EmbedND(\n",
    "        dim=transformer_model.inner_dim, theta=10000, axes_dim=[16, 56, 56]\n",
    "    )\n",
    "\n",
    "    # 替换transformer块\n",
    "    transformer_model.transformer_blocks = torch.nn.ModuleList([NunchakuFluxModel(m)])\n",
    "    transformer_model.single_transformer_blocks = torch.nn.ModuleList([])\n",
    "\n",
    "    def update_params(self: FluxTransformer2DModel, path: str):\n",
    "        if not os.path.exists(path):\n",
    "            hf_repo_id = os.path.dirname(path)\n",
    "            filename = os.path.basename(path)\n",
    "            path = hf_hub_download(repo_id=hf_repo_id, filename=filename)\n",
    "        block = self.transformer_blocks[0]\n",
    "        assert isinstance(block, NunchakuFluxModel)\n",
    "        block.m.load(path, True)\n",
    "\n",
    "    def set_lora_scale(self: FluxTransformer2DModel, scale: float):\n",
    "        block = self.transformer_blocks[0]\n",
    "        assert isinstance(block, NunchakuFluxModel)\n",
    "        block.m.setLoraScale(SVD_RANK, scale)\n",
    "\n",
    "    transformer_model.nunchaku_update_params = types.MethodType(\n",
    "        update_params, transformer_model\n",
    "    )\n",
    "    transformer_model.nunchaku_set_lora_scale = types.MethodType(\n",
    "        set_lora_scale, transformer_model\n",
    "    )\n",
    "\n",
    "    return transformer_model\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "model = inject_transformer(model, m)\n",
    "\n",
    "model = model.to(\"cuda\")\n",
    "\n",
    "from diffusers.pipelines import FluxControlNetPipeline\n",
    "dtype = torch.bfloat16\n",
    "from diffusers import FluxPipeline, FluxTransformer2DModel\n",
    " \n",
    "dtype = torch.bfloat16\n",
    "\n",
    "flux_id = \"black-forest-labs/FLUX.1-dev\"\n",
    "\n",
    "pipeline = FluxPipeline.from_pretrained(\n",
    "                flux_id,\n",
    "                transformer=model,\n",
    "                torch_dtype=dtype,\n",
    "            )\n",
    "\n",
    "pipeline.vae.to(\"cuda\")\n",
    "pipeline.text_encoder.to(\"cuda\")\n",
    "pipeline.text_encoder_2.to(\"cuda\")\n",
    "\n",
    "print(11)\n",
    "\n",
    "image = pipeline(\n",
    "    \"A cat holding a sign that says hello world\",\n",
    "    num_inference_steps=28,\n",
    "    guidance_scale=3.5,\n",
    ").images[0]\n",
    "\n",
    "image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728073dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d069ffa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c58282e-7c99-43b8-b1b8-c6748d659b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cc03a13c9ff4253ab3e429a176d22b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from diffusers import FluxControlNetModel\n",
    "\n",
    "del pipeline\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load pipeline\n",
    "controlnet = FluxControlNetModel.from_pretrained(\n",
    "  \"jasperai/Flux.1-dev-Controlnet-Depth\",\n",
    "  torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "flux_id = \"black-forest-labs/FLUX.1-dev\"\n",
    "\n",
    "pipeline = FluxControlNetPipeline.from_pretrained(\n",
    "                flux_id,\n",
    "                transformer=model,\n",
    "                torch_dtype=dtype,\n",
    "      controlnet=controlnet,\n",
    "\n",
    "            )\n",
    "pipeline.controlnet.to(\"cuda\")\n",
    "print(1)\n",
    "\n",
    "pipeline.vae.to(\"cuda\")\n",
    "pipeline.text_encoder.to(\"cuda\")\n",
    "\n",
    "print(11)\n",
    "\n",
    "from diffusers.utils import load_image\n",
    "\n",
    "\n",
    "control_image = load_image(\n",
    "  \"https://hf-mirror.com/jasperai/Flux.1-dev-Controlnet-Depth/resolve/main/examples/depth.jpg\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "prompt = \"a statue of a gnome in a field of purple tulips\"\n",
    "\n",
    "image = pipeline(\n",
    "    prompt, \n",
    "    control_image=control_image,\n",
    "    controlnet_conditioning_scale=0.6,\n",
    "    num_inference_steps=28, \n",
    "    guidance_scale=3.5,\n",
    "    height=control_image.size[1],\n",
    "    width=control_image.size[0]\n",
    ").images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8afd166-f8d5-429b-b412-b443cccbdeaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459251f6-c901-436d-ac90-6e90ce701665",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
